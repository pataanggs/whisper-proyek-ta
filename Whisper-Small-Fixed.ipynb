{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1648959d",
   "metadata": {},
   "source": [
    "# Fine-tuning Whisper Small (Robust Version)\n",
    "\n",
    "This notebook fine-tunes `openai/whisper-small` on the Minangkabau language.\n",
    "\n",
    "### üõ†Ô∏è Fixes in this version:\n",
    "1.  **WAV Conversion:** Converts all MP3s to WAV before processing to prevent kernel crashes.\n",
    "2.  **Safety Filtering:** Removes audio > 30 seconds to prevent Out-Of-Memory errors.\n",
    "3.  **Evaluation:** Includes WER metrics and WandB logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "!apt-get update -y && apt-get install -y ffmpeg\n",
    "!pip install -q datasets transformers torchaudio evaluate jiwer accelerate tensorboard scikit-learn wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import wandb\n",
    "import subprocess\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"openai/whisper-small\"\n",
    "LANGUAGE = \"minangkabau\"\n",
    "TASK = \"transcribe\"\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"/workspace/data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_DIR = Path(\"/workspace/whisper-minang-finetuned\")\n",
    "LOG_DIR = Path(\"/workspace/logs\")\n",
    "\n",
    "# Login to WandB\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009cdbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Download & Extract Data\n",
    "urls = [\n",
    "    \"https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/resolve/main/data/audio_train.tgz\",\n",
    "    \"https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/resolve/main/data/audio_test.tgz\",\n",
    "    \"https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/resolve/main/data/metadata_train.csv.gz\",\n",
    "    \"https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/resolve/main/data/metadata_test.csv.gz\"\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    dest = DATA_DIR / filename\n",
    "    if not dest.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        !wget -nc {url} -P {DATA_DIR}\n",
    "\n",
    "print(\"Extracting archives...\")\n",
    "for archive in [\"audio_train.tgz\", \"audio_test.tgz\"]:\n",
    "    archive_path = DATA_DIR / archive\n",
    "    # Simple check to see if we extracted already\n",
    "    if not (DATA_DIR / \"audio_train\" / \"librivox-indonesia\").exists():\n",
    "        with tarfile.open(archive_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=DATA_DIR)\n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CRITICAL FIX: Convert Audio to WAV (Prevents crashes)\n",
    "AUDIO_ROOT = DATA_DIR / \"audio_train\" / \"librivox-indonesia\"\n",
    "WAV_DIR = DATA_DIR / \"converted_wav\"\n",
    "WAV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def convert_to_wav(mp3_path):\n",
    "    try:\n",
    "        mp3_path = Path(mp3_path)\n",
    "        relative_path = mp3_path.relative_to(AUDIO_ROOT)\n",
    "        output_path = WAV_DIR / relative_path.with_suffix(\".wav\")\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if output_path.exists():\n",
    "            return str(output_path)\n",
    "\n",
    "        # Convert to 16kHz Mono WAV\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\", \"-v\", \"error\",\n",
    "            \"-i\", str(mp3_path),\n",
    "            \"-ac\", \"1\",\n",
    "            \"-ar\", \"16000\",\n",
    "            str(output_path)\n",
    "        ]\n",
    "        subprocess.run(cmd, check=True)\n",
    "        return str(output_path)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"üîç Scanning for MP3 files...\")\n",
    "mp3_files = list(AUDIO_ROOT.rglob(\"*.mp3\"))\n",
    "print(f\"Found {len(mp3_files)} files. Converting to WAV (this may take a moment)...\")\n",
    "\n",
    "# Run conversion in parallel\n",
    "with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    list(tqdm(executor.map(convert_to_wav, mp3_files), total=len(mp3_files)))\n",
    "\n",
    "print(\"‚úÖ Audio conversion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prepare Metadata & Dataset\n",
    "train_meta_path = DATA_DIR / \"metadata_train.csv.gz\"\n",
    "test_meta_path = DATA_DIR / \"metadata_test.csv.gz\"\n",
    "\n",
    "df_train = pd.read_csv(train_meta_path)\n",
    "df_test = pd.read_csv(test_meta_path)\n",
    "\n",
    "# Filter for Minangkabau\n",
    "df_train = df_train[df_train[\"language\"] == \"min\"].copy()\n",
    "df_test = df_test[df_test[\"language\"] == \"min\"].copy()\n",
    "\n",
    "# Update paths to point to the NEW WAV files\n",
    "def get_wav_path(row):\n",
    "    original_path = Path(row[\"path\"])\n",
    "    # Map original structure to our new WAV_DIR\n",
    "    wav_path = WAV_DIR / original_path.with_suffix(\".wav\")\n",
    "    return str(wav_path)\n",
    "\n",
    "df_train[\"audio\"] = df_train.apply(get_wav_path, axis=1)\n",
    "df_test[\"audio\"] = df_test.apply(get_wav_path, axis=1)\n",
    "\n",
    "# Verify files exist\n",
    "df_train = df_train[df_train[\"audio\"].apply(os.path.exists)]\n",
    "df_test = df_test[df_test[\"audio\"].apply(os.path.exists)]\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Cast to Audio (now using safe WAVs)\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
    "\n",
    "# Train/Test Split\n",
    "train_test_split = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e36157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Filter Long Audio Files (Safety Step)\n",
    "def is_audio_in_length_range(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    return audio[\"array\"].shape[0] < 30 * SAMPLING_RATE\n",
    "\n",
    "print(f\"Original training size: {len(train_dataset)}\")\n",
    "train_dataset = train_dataset.filter(is_audio_in_length_range, num_proc=1)\n",
    "eval_dataset = eval_dataset.filter(is_audio_in_length_range, num_proc=1)\n",
    "print(f\"Filtered training size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Preprocessing\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# We use num_proc=1 to be absolutely safe, but with WAVs you could try os.cpu_count()\n",
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=1)\n",
    "eval_dataset = eval_dataset.map(prepare_dataset, remove_columns=eval_dataset.column_names, num_proc=1)\n",
    "test_dataset_processed = test_dataset.map(prepare_dataset, remove_columns=test_dataset.column_names, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbecb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Data Collator & Metrics\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "metric = evaluate.load(\"jiwer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Training Setup\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.config.use_cache = False\n",
    "model.generation_config.language = LANGUAGE\n",
    "model.generation_config.task = TASK\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    run_name=\"whisper-minangkabau-fixed\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af749c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save & Finish\n",
    "wandb.finish()\n",
    "trainer.save_model(OUTPUT_DIR / \"final_model\")\n",
    "processor.save_pretrained(OUTPUT_DIR / \"final_model\")\n",
    "print(\"Training Complete and Model Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80423729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Final Evaluation\n",
    "print(\"Evaluating on Test Set...\")\n",
    "test_metrics = trainer.evaluate(test_dataset_processed)\n",
    "print(f\"Test WER: {test_metrics['eval_wer']:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
