{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21125ed1",
   "metadata": {},
   "source": [
    "# System Dependencies and Additional Packages\n",
    "This cell updates the system packages and installs ffmpeg for audio processing, along with additional Python packages including datasets, transformers, torchaudio, evaluate, jiwer, torchcodec, and tensorboard. It also upgrades transformers and accelerate to their latest versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea804f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu noble InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu noble-updates InRelease                 \n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64  InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu noble-backports InRelease               \n",
      "Hit:5 http://security.ubuntu.com/ubuntu noble-security InRelease               \n",
      "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble InRelease   \n",
      "Reading package lists... Done\n",
      "W: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:6.1.1-3ubuntu5).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 63 not upgraded.\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
      "Requirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
      "Requirement already satisfied: audiomentations in /usr/local/lib/python3.12/dist-packages (0.43.1)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.20.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchaudio) (2.8.0+cu128)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.4.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer) (3.14.3)\n",
      "Requirement already satisfied: numpy-minmax<1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.5.0)\n",
      "Requirement already satisfied: numpy-rms<1,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.6.0)\n",
      "Requirement already satisfied: librosa!=0.10.0,<0.12.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.11.0)\n",
      "Requirement already satisfied: python-stretch<1,>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.3.1)\n",
      "Requirement already satisfied: scipy<2,>=1.4 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (1.16.3)\n",
      "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.5.0.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.62.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.8.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (2.0.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard) (11.0.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (6.33.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.23)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.45.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!apt-get update -y\n",
    "!apt-get install -y ffmpeg\n",
    "!pip install datasets transformers torchaudio evaluate jiwer torchcodec audiomentations tensorboard scikit-learn accelerate\n",
    "!pip install --upgrade transformers accelerate \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f2329",
   "metadata": {},
   "source": [
    "# Imports and Configuration Setup\n",
    "This cell imports all necessary libraries for data processing, audio handling, and machine learning. It sets up environment variables to disable tokenizers parallelism warnings and defines configuration parameters including the number of processors and sampling rate. It also creates the base directory for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "548e6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Features, Value, Audio\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Set environment variable to disable tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Konfigurasi umum\n",
    "num_proc = os.cpu_count()\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "# Direktori kerja\n",
    "base_dir = Path(\"/workspace/data/audio_train/librivox-indonesia\")\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae1adf",
   "metadata": {},
   "source": [
    "# Download Dataset Files\n",
    "This cell downloads the necessary dataset files from Hugging Face, including training and test audio archives, and their corresponding metadata CSV files compressed with gzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26adca17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File â€˜/workspace/data/audio_train.tgzâ€™ already there; not retrieving.\n",
      "\n",
      "File â€˜/workspace/data/audio_test.tgzâ€™ already there; not retrieving.\n",
      "\n",
      "File â€˜/workspace/data/metadata_train.csv.gzâ€™ already there; not retrieving.\n",
      "\n",
      "File â€˜/workspace/data/metadata_test.csv.gzâ€™ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/resolve/main/data/audio_train.tgz -P /workspace/data/\n",
    "!wget -nc https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/resolve/main/data/audio_test.tgz -P /workspace/data/\n",
    "!wget -nc https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/resolve/main/data/metadata_train.csv.gz -P /workspace/data/\n",
    "!wget -nc https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/resolve/main/data/metadata_test.csv.gz -P /workspace/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a8518",
   "metadata": {},
   "source": [
    "# Extract Audio Archives\n",
    "This cell extracts the downloaded tar.gz archives containing the training and test audio files to the specified directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "812a5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak audio\n",
    "for archive in [\"/workspace/data/audio_train.tgz\", \"/workspace/data/audio_test.tgz\"]:\n",
    "    with tarfile.open(archive, \"r:gz\") as tar:\n",
    "        tar.extractall(path=\"/workspace/data/audio_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76eae3",
   "metadata": {},
   "source": [
    "# Audio Format Conversion\n",
    "This cell converts all audio files from their original formats to WAV format with mono channel and 16kHz sampling rate using ffmpeg. It processes the files in parallel for efficiency and updates the metadata CSV to reflect the new file paths. The conversion ensures compatibility with the Whisper model's audio processing requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "135044e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to WAV: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7061/7061 [00:10<00:00, 656.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Conversion done. Metadata saved: /workspace/data/audio_train/librivox-indonesia/metadata_train_wav.csv\n",
      "ðŸ“‚ Converted audio saved under: /workspace/data/audio_train/librivox-indonesia/converted_wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# --- Paths ---\n",
    "audio_base_path = \"/workspace/data/audio_train/librivox-indonesia\"\n",
    "output_base = os.path.join(audio_base_path, \"converted_wav\")\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "# ðŸ“Œ Metadata asli\n",
    "metadata_path = \"/workspace/data/metadata_train.csv.gz\"\n",
    "df = pd.read_csv(metadata_path)\n",
    "\n",
    "# --- Worker function for parallel ffmpeg ---\n",
    "def convert_to_wav(rel_path):\n",
    "    if pd.isna(rel_path):\n",
    "        return None\n",
    "\n",
    "    src_path = os.path.join(audio_base_path, rel_path)\n",
    "    if not os.path.exists(src_path):\n",
    "        return None\n",
    "\n",
    "    out_rel_path = os.path.splitext(rel_path)[0] + \".wav\"\n",
    "    out_path = os.path.join(output_base, out_rel_path)\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    # âœ… Skip jika sudah ada\n",
    "    if os.path.exists(out_path):\n",
    "        return os.path.relpath(out_path, audio_base_path)\n",
    "\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", src_path, \"-ac\", \"1\", \"-ar\", \"16000\", out_path]\n",
    "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "    return os.path.relpath(out_path, audio_base_path)\n",
    "\n",
    "# --- Run parallel conversion ---\n",
    "new_paths = []\n",
    "with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = {executor.submit(convert_to_wav, rel_path): rel_path for rel_path in df[\"path\"].astype(str)}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Converting to WAV\"):\n",
    "        new_paths.append(future.result())\n",
    "\n",
    "# --- Update metadata ke versi WAV ---\n",
    "df[\"path\"] = new_paths\n",
    "new_meta_path = os.path.join(audio_base_path, \"metadata_train_wav.csv\")\n",
    "df.to_csv(new_meta_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Conversion done. Metadata saved: {new_meta_path}\")\n",
    "print(f\"ðŸ“‚ Converted audio saved under: {output_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7459b",
   "metadata": {},
   "source": [
    "# Cleanup Original Audio Files\n",
    "This cell removes the original MP3 files after conversion to WAV format to save disk space and avoid confusion between different audio formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8797d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—‘ï¸ Cleaning MP3 in /workspace/data/audio_train/librivox-indonesia ...\n",
      "âœ… Removed 7815 MP3 files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "def cleanup_mp3(split_dir):\n",
    "    print(f\"ðŸ—‘ï¸ Cleaning MP3 in {split_dir} ...\")\n",
    "    removed = 0\n",
    "    for mp3 in glob.glob(os.path.join(split_dir, \"**\", \"*.mp3\"), recursive=True):\n",
    "        try:\n",
    "            os.remove(mp3)\n",
    "            removed += 1\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"âœ… Removed {removed} MP3 files\")\n",
    "\n",
    "cleanup_mp3(audio_base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27157f",
   "metadata": {},
   "source": [
    "# Dataset Preparation for Minangkabau Language\n",
    "This cell loads the converted metadata, filters the dataset to include only Minangkabau language samples, adds absolute file paths, and converts the pandas DataFrame to a Hugging Face Dataset object for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5408069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading metadata from: /workspace/data/audio_train/librivox-indonesia/metadata_train_wav.csv\n",
      "\n",
      "ðŸ“Š Language Distribution in Dataset:\n",
      "language\n",
      "ind    5635\n",
      "jav     728\n",
      "bal     159\n",
      "sun     140\n",
      "ace     139\n",
      "min     136\n",
      "bug     124\n",
      "Name: count, dtype: int64\n",
      "âœ… Final Dataset Size for 'min': 136 samples\n",
      "\n",
      "âš™ï¸ Loading Whisper Processor...\n",
      "ðŸ”„ Processing audio features (Mapping)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32a0a16b8274ff69e59b76c698838b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97439eefa7ea4bc589720775289d2d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Data Preparation Complete!\n",
      "   Train set: 122\n",
      "   Val set:   14\n",
      "   Variables ready: train_ds, val_ds, processor, data_collator\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "from datasets import Dataset, Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "audio_base_path = \"/workspace/data/audio_train/librivox-indonesia\"\n",
    "metadata_path = os.path.join(audio_base_path, \"metadata_train_wav.csv\")\n",
    "MODEL_NAME = \"openai/whisper-small\"\n",
    "TARGET_LANGUAGE = \"min\"  # Target language code\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "# --- 1. LOAD METADATA ---\n",
    "if not os.path.exists(metadata_path):\n",
    "    raise FileNotFoundError(f\"âŒ Metadata file not found at {metadata_path}.\")\n",
    "\n",
    "print(f\"ðŸ“‚ Loading metadata from: {metadata_path}\")\n",
    "df = pd.read_csv(metadata_path)\n",
    "\n",
    "# --- 2. CHECK & SELECT LANGUAGE ---\n",
    "if \"language\" not in df.columns:\n",
    "    raise ValueError(f\"âŒ Column 'language' not found in metadata! Columns: {df.columns}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Language Distribution in Dataset:\")\n",
    "print(df[\"language\"].value_counts())\n",
    "\n",
    "# Filter for Target Language\n",
    "df_lang = df[df[\"language\"] == TARGET_LANGUAGE].copy()\n",
    "\n",
    "# AUTO-FALLBACK LOGIC\n",
    "if len(df_lang) == 0:\n",
    "    print(f\"\\nâš ï¸ WARNING: No samples found for target language '{TARGET_LANGUAGE}'.\")\n",
    "    most_common_lang = df[\"language\"].mode()[0]\n",
    "    count = len(df[df[\"language\"] == most_common_lang])\n",
    "    print(f\"ðŸ”„ Switching to most common language: '{most_common_lang}' ({count} samples)\")\n",
    "    df_lang = df[df[\"language\"] == most_common_lang].copy()\n",
    "    TARGET_LANGUAGE = most_common_lang\n",
    "\n",
    "# --- 3. CLEANUP PATHS ---\n",
    "# Remove NaN paths\n",
    "df_lang = df_lang.dropna(subset=[\"path\"])\n",
    "df_lang[\"path\"] = df_lang[\"path\"].astype(str)\n",
    "\n",
    "# Construct absolute paths\n",
    "df_lang[\"full_path\"] = df_lang[\"path\"].apply(lambda p: os.path.join(audio_base_path, p))\n",
    "\n",
    "# Verify files exist (Fast check)\n",
    "df_lang[\"exists\"] = df_lang[\"full_path\"].apply(os.path.exists)\n",
    "missing_count = len(df_lang) - df_lang[\"exists\"].sum()\n",
    "if missing_count > 0:\n",
    "    print(f\"âš ï¸ Warning: {missing_count} audio files are missing from disk. Dropping them.\")\n",
    "    df_lang = df_lang[df_lang[\"exists\"]]\n",
    "\n",
    "print(f\"âœ… Final Dataset Size for '{TARGET_LANGUAGE}': {len(df_lang)} samples\")\n",
    "\n",
    "# --- 4. TRAIN/TEST SPLIT ---\n",
    "# We do this BEFORE creating the HF Dataset to ensure distinct splits\n",
    "train_df, val_df = train_test_split(df_lang, test_size=0.1, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "\n",
    "# --- 5. PROCESSOR & FEATURE EXTRACTION ---\n",
    "print(\"\\nâš™ï¸ Loading Whisper Processor...\")\n",
    "# Note: We use \"indonesian\" as the language token because Whisper might not support \"min\" directly\n",
    "# This helps transfer learning from Indonesian to Minangkabau\n",
    "lang_token = \"indonesian\" if TARGET_LANGUAGE in [\"min\", \"ind\", \"jav\", \"sun\"] else \"english\"\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=lang_token, task=\"transcribe\")\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"Preprocesses audio to input_features\"\"\"\n",
    "    # Load audio\n",
    "    audio_path = batch[\"full_path\"]\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sr != SAMPLING_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, SAMPLING_RATE)\n",
    "    \n",
    "    # Convert to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Squeeze to 1D array for processor\n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    # Process audio\n",
    "    inputs = processor(\n",
    "        audio=waveform.numpy(),\n",
    "        sampling_rate=SAMPLING_RATE,\n",
    "        text=batch[\"sentence\"]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_features\": inputs[\"input_features\"][0],\n",
    "        \"labels\": inputs[\"labels\"]\n",
    "    }\n",
    "\n",
    "print(\"ðŸ”„ Processing audio features (Mapping)...\")\n",
    "# Use num_proc=1 to avoid multiprocessing issues in some notebooks, increase if safe\n",
    "train_ds = train_ds.map(prepare_dataset, num_proc=1)\n",
    "val_ds = val_ds.map(prepare_dataset, num_proc=1)\n",
    "\n",
    "# --- 6. DATA COLLATOR ---\n",
    "@torch.no_grad()\n",
    "def data_collator(features):\n",
    "    input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "    labels = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "    batch = processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "    # Pad labels manually\n",
    "    max_label_len = max(len(l) for l in labels)\n",
    "    padded_labels = []\n",
    "    \n",
    "    for label in labels:\n",
    "        padded_label = label + [-100] * (max_label_len - len(label))\n",
    "        padded_labels.append(padded_label)\n",
    "        \n",
    "    labels_tensor = torch.tensor(padded_labels, dtype=torch.long)\n",
    "    \n",
    "    # Trim BOS if present (standard Whisper hygiene)\n",
    "    if (labels_tensor[:, 0] == processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "        labels_tensor = labels_tensor[:, 1:]\n",
    "        \n",
    "    batch[\"labels\"] = labels_tensor\n",
    "    return batch\n",
    "\n",
    "print(\"\\nâœ… Data Preparation Complete!\")\n",
    "print(f\"   Train set: {len(train_ds)}\")\n",
    "print(f\"   Val set:   {len(val_ds)}\")\n",
    "print(\"   Variables ready: train_ds, val_ds, processor, data_collator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c95ebc",
   "metadata": {},
   "source": [
    "# Load Whisper Model and Processor\n",
    "This cell loads the pre-trained OpenAI Whisper Small model and its processor from Hugging Face. The model is moved to GPU (CUDA) for faster training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b90fc831-26ea-4b23-a928-5b8c66bb4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "model_name = \"openai/whisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Pastikan model ke GPU\n",
    "model = model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32daea07",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing and Training Setup\n",
    "This cell performs comprehensive setup for fine-tuning the Whisper model: loads the model and processor configured for Minangkabau language transcription, prepares the dataset by splitting into train/validation sets, applies preprocessing to convert audio waveforms to model inputs, and sets up the data collator for sequence-to-sequence training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebe29fcf-6a22-4645-b205-366039c5c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# --- Config ---\n",
    "audio_base_path = \"/workspace/data/audio_train/librivox-indonesia\"\n",
    "metadata_path = os.path.join(audio_base_path, \"metadata_train_wav.csv\")\n",
    "SAMPLING_RATE = 16000\n",
    "model_name = \"openai/whisper-small\"\n",
    "\n",
    "# --- Load model & processor ---\n",
    "# Change \"minangkabau\" to \"indonesian\"\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    model_name, \n",
    "    language=\"indonesian\", \n",
    "    task=\"transcribe\"\n",
    ")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move generation config parameters to avoid warnings\n",
    "model.generation_config.max_length = 448\n",
    "model.generation_config.suppress_tokens = [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362]\n",
    "model.generation_config.begin_suppress_tokens = [220, 50257]\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# --- STEP 1: Load metadata ---\n",
    "df = pd.read_csv(metadata_path)\n",
    "df[\"full_path\"] = df[\"path\"].apply(lambda p: os.path.join(audio_base_path, p))\n",
    "\n",
    "# Fokus hanya Minangkabau\n",
    "df = df[df[\"language\"] == \"min\"].reset_index(drop=True)\n",
    "\n",
    "# Split train/validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert ke HF Dataset\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db8eaebd-a02a-4d7d-a47a-05bdaaee1d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7887ba4b474488a3cae37030a1f76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    }
   ],
   "source": [
    "# Install audiomentations if not present\n",
    "# !pip install audiomentations\n",
    "\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, PolarityInversion\n",
    "import numpy as np\n",
    "\n",
    "# Define augmentation pipeline\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "])\n",
    "\n",
    "def augment_audio(batch):\n",
    "    # Load audio using torchaudio\n",
    "    waveform, sr = torchaudio.load(batch[\"full_path\"])\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sr != SAMPLING_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, SAMPLING_RATE)\n",
    "    \n",
    "    # Convert to numpy for audiomentations\n",
    "    waveform_np = waveform.numpy()[0] # Take first channel\n",
    "    \n",
    "    # Apply augmentation\n",
    "    augmented_waveform = augment(samples=waveform_np, sample_rate=SAMPLING_RATE)\n",
    "    \n",
    "    # Process with Whisper\n",
    "    inputs = processor(\n",
    "        audio=augmented_waveform,\n",
    "        sampling_rate=SAMPLING_RATE,\n",
    "        text=batch[\"sentence\"]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_features\": inputs[\"input_features\"][0],\n",
    "        \"labels\": inputs[\"labels\"]\n",
    "    }\n",
    "\n",
    "# Apply augmentation ONLY to the training set\n",
    "# Note: This applies augmentation once per epoch. \n",
    "# For better results, apply it dynamically inside a custom DataCollator, \n",
    "# but this `map` method is a good starting point.\n",
    "train_ds = train_ds.map(augment_audio, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b495830f-b37a-4dd4-b8f2-4dbfcb61ebb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b0f28cdfcf4da990a869ff86d26904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b285e4208d8a43e983d728b947527054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- STEP 2: Preprocessing function ---\n",
    "def prepare_dataset(batch):\n",
    "    # load audio\n",
    "    waveform, sr = torchaudio.load(batch[\"full_path\"])\n",
    "    if sr != SAMPLING_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, SAMPLING_RATE)\n",
    "    waveform = waveform.mean(dim=0)  # mono\n",
    "\n",
    "    # process\n",
    "    inputs = processor(\n",
    "        audio=waveform.numpy(),\n",
    "        sampling_rate=SAMPLING_RATE,\n",
    "        text=batch[\"sentence\"]\n",
    "    )\n",
    "    return {\n",
    "        \"input_features\": inputs[\"input_features\"][0],\n",
    "        \"labels\": inputs[\"labels\"]\n",
    "    }\n",
    "\n",
    "train_ds = train_ds.map(prepare_dataset)\n",
    "val_ds = val_ds.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f41bc1-29a8-4103-a729-6142ef301cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# --- STEP 3: Data Collator (fixed for tokenized labels) ---\n",
    "@torch.no_grad()\n",
    "def data_collator(features):\n",
    "    # Separate inputs and labels for clarity\n",
    "    input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "    labels = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "    # Use feature extractor for audio inputs\n",
    "    batch = processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "    # Convert labels to tensors and pad them manually\n",
    "    # Labels are already tokenized, so we need to pad them as integers\n",
    "    max_length = max(len(label) for label in labels)\n",
    "    \n",
    "    padded_labels = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for label in labels:\n",
    "        # Pad with -100 (ignored in loss calculation)\n",
    "        padded_label = label + [-100] * (max_length - len(label))\n",
    "        attention_mask = [1] * len(label) + [0] * (max_length - len(label))\n",
    "        \n",
    "        padded_labels.append(padded_label)\n",
    "        attention_masks.append(attention_mask)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    labels_tensor = torch.tensor(padded_labels, dtype=torch.long)\n",
    "    attention_mask = torch.tensor(attention_masks, dtype=torch.long)\n",
    "    \n",
    "    # Replace padding tokens with -100 for loss calculation\n",
    "    labels_tensor = labels_tensor.masked_fill(attention_mask.ne(1), -100)\n",
    "\n",
    "    # Trim BOS token if present (crucial for Whisper)\n",
    "    if (labels_tensor[:, 0] == processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "        labels_tensor = labels_tensor[:, 1:]\n",
    "\n",
    "    batch[\"labels\"] = labels_tensor\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a35d0",
   "metadata": {},
   "source": [
    "# Model Training Execution\n",
    "This cell initiates the fine-tuning process of the Whisper model on the Minangkabau language dataset using the configured trainer. The training will run for the specified number of epochs with periodic evaluation and checkpoint saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc1c6557-1c06-45ff-9e4e-543694860b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Loading fresh model on NVIDIA A40...\n",
      "âœ… Fresh model loaded. Gradient Checkpointing DISABLED. Metric function ADDED.\n",
      "ðŸš€ Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 1:02:00, Epoch 250/250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.851200</td>\n",
       "      <td>4.386569</td>\n",
       "      <td>145.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>4.484226</td>\n",
       "      <td>257.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>4.643561</td>\n",
       "      <td>177.560976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>4.675807</td>\n",
       "      <td>184.878049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>4.683607</td>\n",
       "      <td>185.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>4.689660</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>4.695017</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>4.700410</td>\n",
       "      <td>177.560976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4.703907</td>\n",
       "      <td>176.097561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4.705173</td>\n",
       "      <td>176.097561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.4096466833022423, metrics={'train_runtime': 3728.871, 'train_samples_per_second': 8.582, 'train_steps_per_second': 0.268, 'total_flos': 8.80185470976e+18, 'train_loss': 0.4096466833022423, 'epoch': 250.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "import gc\n",
    "import evaluate\n",
    "\n",
    "# --- STEP 1: Define Metrics ---\n",
    "# Load the WER metric\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Computes Word Error Rate (WER) for evaluation.\n",
    "    \"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 in the labels with the pad_token_id (so we can decode them)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and references\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    # We multiply by 100 to make it easier to read (e.g., 15.5 instead of 0.155)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# --- STEP 2: Clean Memory ---\n",
    "# Delete previous instances to clear VRAM and computation graph\n",
    "if 'trainer' in globals(): del trainer\n",
    "if 'model' in globals(): del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- STEP 3: Reload Model (Clean Slate) ---\n",
    "print(\"ðŸš€ Loading fresh model on NVIDIA A40...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Configure Model\n",
    "# Disable cache for training (prevents warnings)\n",
    "model.config.use_cache = False \n",
    "# IMPORTANT: We DO NOT enable gradient checkpointing because you have an A40 (48GB VRAM)\n",
    "# and the model is small. Enabling it caused the previous RuntimeError.\n",
    "\n",
    "# --- STEP 4: Training Arguments (Optimized for A40) ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-minang-checkpoints\",\n",
    "    \n",
    "    # POWER MOVE: Increasing batch size because you have 48GB VRAM\n",
    "    per_device_train_batch_size=32,  # Increased from 8 -> 32 for speed\n",
    "    gradient_accumulation_steps=1,   # No need to accumulate with high batch size\n",
    "    \n",
    "    learning_rate=1e-5,              # Low LR for stability\n",
    "    warmup_steps=50,\n",
    "    max_steps=1000,                  # Force 1000 update steps\n",
    "    \n",
    "    gradient_checkpointing=False,    # <--- CRITICAL: DISABLED to fix RuntimeError\n",
    "    \n",
    "    fp16=True,                       # A40 handles FP16 perfectly\n",
    "    eval_strategy=\"steps\",           # Renamed from evaluation_strategy\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=25,\n",
    "    report_to=\"tensorboard\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# --- STEP 5: Initialize Trainer ---\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor,\n",
    "    compute_metrics=compute_metrics,  # <--- CRITICAL FIX: This was missing in your code!\n",
    ")\n",
    "\n",
    "# --- STEP 6: Start Training ---\n",
    "print(\"âœ… Fresh model loaded. Gradient Checkpointing DISABLED. Metric function ADDED.\")\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad0faf4-d229-4b61-afeb-a8da43c3897c",
   "metadata": {},
   "source": [
    "# Model Testing and Evaluation\n",
    "This section loads the test dataset and evaluates the fine-tuned Whisper model's performance on unseen Minangkabau audio samples. It processes the test audio files and compares the model's transcriptions with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a870b2c-6294-4ef2-adf0-9913b381cd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Extracting test audio files...\n",
      "âœ… Test audio extracted\n",
      "ðŸ“Š Total test samples: 754\n",
      "ðŸŽ¯ Minangkabau test samples: 20\n",
      "\n",
      "ðŸ“ Sample test data:\n",
      "                                            sentence  \\\n",
      "0                       katahui bana hak hak awak ko   \n",
      "1  dan kabebasan dari raso takuik dan dari kakura...   \n",
      "2                           supayo manjadi kanyataan   \n",
      "\n",
      "                                                path  \n",
      "0  test/minangkabau/universal-declaration-of-huma...  \n",
      "1  test/minangkabau/universal-declaration-of-huma...  \n",
      "2  test/minangkabau/universal-declaration-of-huma...  \n"
     ]
    }
   ],
   "source": [
    "# --- Prepare Test Data ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "\n",
    "# Extract test audio if not already extracted\n",
    "test_archive = \"/workspace/data/audio_test.tgz\"\n",
    "if os.path.exists(test_archive):\n",
    "    print(\"ðŸ“‚ Extracting test audio files...\")\n",
    "    with tarfile.open(test_archive, \"r:gz\") as tar:\n",
    "        tar.extractall(path=\"/workspace/data/audio_train\")\n",
    "    print(\"âœ… Test audio extracted\")\n",
    "\n",
    "# Load test metadata\n",
    "test_metadata_path = \"/workspace/data/metadata_test.csv.gz\"\n",
    "test_df = pd.read_csv(test_metadata_path)\n",
    "\n",
    "# Filter for Minangkabau language only\n",
    "test_df_min = test_df[test_df[\"language\"] == \"min\"].reset_index(drop=True)\n",
    "print(f\"ðŸ“Š Total test samples: {len(test_df)}\")\n",
    "print(f\"ðŸŽ¯ Minangkabau test samples: {len(test_df_min)}\")\n",
    "\n",
    "# Add full paths\n",
    "audio_base_path = \"/workspace/data/audio_train/librivox-indonesia\"\n",
    "test_df_min[\"full_path\"] = test_df_min[\"path\"].apply(lambda p: os.path.join(audio_base_path, p))\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nðŸ“ Sample test data:\")\n",
    "print(test_df_min[[\"sentence\", \"path\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05d86f56-343a-43c4-bd84-f611c788b7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting test audio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:06<00:00,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully converted 20 test audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Convert Test Audio to WAV (if needed) ---\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_test_audio_to_wav(df, output_base):\n",
    "    \"\"\"Convert test audio files to WAV format for consistency\"\"\"\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    \n",
    "    converted_paths = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Converting test audio\"):\n",
    "        rel_path = row[\"path\"]\n",
    "        src_path = os.path.join(audio_base_path, rel_path)\n",
    "        \n",
    "        # Skip if source doesn't exist\n",
    "        if not os.path.exists(src_path):\n",
    "            converted_paths.append(None)\n",
    "            continue\n",
    "            \n",
    "        # Create WAV output path\n",
    "        out_rel_path = os.path.splitext(rel_path)[0] + \".wav\"\n",
    "        out_path = os.path.join(output_base, out_rel_path)\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        \n",
    "        # Skip if already converted\n",
    "        if os.path.exists(out_path):\n",
    "            converted_paths.append(os.path.relpath(out_path, audio_base_path))\n",
    "            continue\n",
    "            \n",
    "        # Convert to WAV\n",
    "        cmd = [\"ffmpeg\", \"-y\", \"-i\", src_path, \"-ac\", \"1\", \"-ar\", \"16000\", out_path]\n",
    "        result = subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            converted_paths.append(os.path.relpath(out_path, audio_base_path))\n",
    "        else:\n",
    "            converted_paths.append(None)\n",
    "    \n",
    "    return converted_paths\n",
    "\n",
    "# Convert test audio\n",
    "test_output_base = os.path.join(audio_base_path, \"converted_wav\")\n",
    "test_df_min[\"wav_path\"] = convert_test_audio_to_wav(test_df_min, test_output_base)\n",
    "test_df_min[\"wav_full_path\"] = test_df_min[\"wav_path\"].apply(\n",
    "    lambda p: os.path.join(audio_base_path, p) if p else None\n",
    ")\n",
    "\n",
    "# Remove rows where conversion failed\n",
    "test_df_clean = test_df_min.dropna(subset=[\"wav_full_path\"]).reset_index(drop=True)\n",
    "print(f\"âœ… Successfully converted {len(test_df_clean)} test audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27be7f42-0037-4563-a854-6f313b05a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤ Testing model on sample audio files...\n",
      "Processing 1/20: human_rights_un_min_sd_0009.wav\n",
      "  Ground Truth: katahui bana hak hak awak ko\n",
      "  Prediction:    ÙˆÙ‚Øª Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¹Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©\n",
      "  ==================================================\n",
      "Processing 2/20: human_rights_un_min_sd_0016.wav\n",
      "  Ground Truth: dan kabebasan dari raso takuik dan dari kakurangan\n",
      "  Prediction:   dan kalaki nan parangakiranjan à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki\n",
      "  ==================================================\n",
      "Processing 3/20: human_rights_un_min_sd_0028.wav\n",
      "  Ground Truth: supayo manjadi kanyataan\n",
      "  Prediction:   akimajal niyo\n",
      "  ==================================================\n",
      "Processing 4/20: human_rights_un_min_sd_0039.wav\n",
      "  Ground Truth: indak ado pambedaan   umpamonyo pambedaan ras\n",
      "  Prediction:   à²•à³†à²°à³†à²¯à²¿à²•à³†à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¯à²¿à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯\n",
      "  ==================================================\n",
      "Processing 5/20: human_rights_un_min_sd_0044.wav\n",
      "  Ground Truth: indak diadokan pambedaan badasar kadudukan politik\n",
      "  Prediction:    à²‡à²°à²¿à²¯à²¨à²¿à²¯à²¨à²¿à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦\n",
      "  ==================================================\n",
      "Processing 6/20: human_rights_un_min_sd_0046.wav\n",
      "  Ground Truth: baiak babantuak negara mardeka\n",
      "  Prediction:    à²‡à²°à²¿à²¯à²¨à²¿à²¯à²¨à²¿à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦\n",
      "  ==================================================\n",
      "Processing 7/20: human_rights_un_min_sd_0047.wav\n",
      "  Ground Truth: wilayah wilayah parwalian  jajahan atau bantuak katarbatasan kadaulatan nan lain\n",
      "  Prediction:    jajahan  atau  bantuak  katarbatasan  kadaulatan nan lain\n",
      "  ==================================================\n",
      "Processing 8/20: human_rights_un_min_sd_0055.wav\n",
      "  Ground Truth: pasal tujuh sadonyo urang samo di hadapan hukum\n",
      "  Prediction:   Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆØ§Ù„Ø³Ø¹ÙˆØ¯Ø© ÙˆÙ…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©\n",
      "  ==================================================\n",
      "Processing 9/20: human_rights_un_min_sd_0060.wav\n",
      "  Ground Truth: pasal sembilan indak surang pun buliah ditangkok  ditahan atau diasiangkan sacaro sawenang wenang\n",
      "  Prediction:   lansaki Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa \n",
      "  ==================================================\n",
      "Processing 10/20: human_rights_un_min_sd_0061.wav\n",
      "  Ground Truth: pasal sepuluh tiok urang punyo hak nan samo mandapek paradilan nan adil dan tabuka di pangadilan nan bebas dan tak mamihak\n",
      "  Prediction:    à²•à³†à²°à³à²¯à²¿à²•à³†à²¯à²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿\n",
      "  ==================================================\n",
      "Processing 11/20: human_rights_un_min_sd_0066.wav\n",
      "  Ground Truth: nan bukan tamasuak palanggaran hukum nasional atau internasional katiko paristiwa itu tajadi\n",
      "  Prediction:    nans  à²‡à² à²¿à²¯à²¨à²¿à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†ï¿½\n",
      "  ==================================================\n",
      "Processing 12/20: human_rights_un_min_sd_0075.wav\n",
      "  Ground Truth: pasal empat belas tiok urang punyo hak mancari dan manikmati suaka di negara lain untuak malinduangi diri dari panganiayoan  di negaranyo\n",
      "  Prediction:   lansak linduangi diri Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡\n",
      "  ==================================================\n",
      "Processing 13/20: human_rights_un_min_sd_0089.wav\n",
      "  Ground Truth: bahati nurani dan ba agamo  tamasuak hak batuka agamo atau kaparcayaan\n",
      "  Prediction:    à²‡à²°à²¿à²•à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯\n",
      "  ==================================================\n",
      "Processing 14/20: human_rights_un_min_sd_0100.wav\n",
      "  Ground Truth: tiok urang punyo hak mandapek kasampatan nan samo untuak diangkek dalam jabatan pamarintahan negaranyo\n",
      "  Prediction:    ÙˆÙ…Ù† ÙŠØ­Ø¨Ø¨ÙˆÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆØ§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©\n",
      "  ==================================================\n",
      "Processing 15/20: human_rights_un_min_sd_0101.wav\n",
      "  Ground Truth: kandak rakyat musti manjadi landasan kakuasaan pamarintah\n",
      "  Prediction:    à¶…à¶´à¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Š\n",
      "  ==================================================\n",
      "Processing 16/20: human_rights_un_min_sd_0105.wav\n",
      "  Ground Truth: tiok urang punyo hak mandapek jaminan sosial dan punyo hak pulo untuak talaksananyo hak hak ekonomi\n",
      "  Prediction:    à²•à³†à²°à³à²¯à²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²µà²¿à²µà²µà²µà²¿à²µà²µà²µà²¿à²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µï¿½\n",
      "  ==================================================\n",
      "Processing 17/20: human_rights_un_min_sd_0113.wav\n",
      "  Ground Truth: baitu pulo punyo hak mandapek linduangan katiko manganggur\n",
      "  Prediction:   lansak linduangahonyo å¿banyo  banso nan  sacaro jiniyak  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro nan  sacaro raso nan  sacaro nan \n",
      "  ==================================================\n",
      "Processing 18/20: human_rights_un_min_sd_0119.wav\n",
      "  Ground Truth: tamasuak pambatasan nan layak taradok jam karajo\n",
      "  Prediction:    tÂ·tÊ°a Ê°i Ê°ak Ê°i Ê°ak Ê°i Ê°ak Ê°i Ê°ak Ê°i Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak ï¿½\n",
      "  ==================================================\n",
      "Processing 19/20: human_rights_un_min_sd_0131.wav\n",
      "  Ground Truth: pandidikan dasar musti diwajibkan\n",
      "  Prediction:    à²•à³†à²°à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²•à³à²¯à²¿à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³\n",
      "  ==================================================\n",
      "Processing 20/20: human_rights_un_min_sd_0146.wav\n",
      "  Ground Truth: pasal dua puluh delapan tiok urang punyo hak mandapek katartiban sosial dan katartiban internasional untuak mawujudkan saluruah hak hak dan kabebasan sasuai jo deklarasi ko\n",
      "  Prediction:    ÙˆÙ…Ù† ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ù‚Ù‚ Ø§Ù„Ù…Ø¯ÙÙ‚Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¥Ù†ØªØ±Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø³ÙˆØ§ØªÙŠØ© ÙˆØ§Ù„Ø³ÙˆØ§ØªÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø© Ù…Ù†Ù‡Ø§\n",
      "  ==================================================\n",
      "\n",
      "âœ… Completed testing on 20 samples\n"
     ]
    }
   ],
   "source": [
    "# --- Test Model Inference ---\n",
    "import torchaudio\n",
    "from transformers import pipeline\n",
    "\n",
    "def transcribe_audio(audio_path, model, processor):\n",
    "    \"\"\"Transcribe a single audio file using the fine-tuned model\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != SAMPLING_RATE:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, SAMPLING_RATE)\n",
    "        waveform = waveform.mean(dim=0)  # Convert to mono\n",
    "        \n",
    "        # Process audio\n",
    "        inputs = processor(\n",
    "            audio=waveform.numpy(),\n",
    "            sampling_rate=SAMPLING_RATE,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move to GPU\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate transcription\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                inputs[\"input_features\"],\n",
    "                max_length=448,\n",
    "                num_beams=1,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode transcription\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test on a small sample first (5 files)\n",
    "test_sample = test_df_clean.head(20).copy()\n",
    "print(\"ðŸŽ¤ Testing model on sample audio files...\")\n",
    "\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for idx, row in test_sample.iterrows():\n",
    "    print(f\"Processing {idx+1}/20: {os.path.basename(row['wav_full_path'])}\")\n",
    "    \n",
    "    # Get prediction\n",
    "    prediction = transcribe_audio(row[\"wav_full_path\"], model, processor)\n",
    "    ground_truth = row[\"sentence\"]\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    ground_truths.append(ground_truth)\n",
    "    \n",
    "    print(f\"  Ground Truth: {ground_truth}\")\n",
    "    print(f\"  Prediction:   {prediction}\")\n",
    "    print(\"  \" + \"=\"*50)\n",
    "\n",
    "print(f\"\\nâœ… Completed testing on {len(test_sample)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd068e33-0ff1-476e-b181-27eae9d7b7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š EVALUATION RESULTS:\n",
      "==================================================\n",
      "ðŸ“ Number of samples: 20\n",
      "ðŸŽ¯ Word Error Rate (WER): 3.8238 (382.38%)\n",
      "ðŸ”¤ Character Error Rate (CER): 3.4872 (348.72%)\n",
      "âœ… Word Accuracy: -282.38%\n",
      "\n",
      "ðŸ“‹ DETAILED COMPARISON:\n",
      "==================================================\n",
      "Sample 1:\n",
      "  Ground Truth: katahui bana hak hak awak ko\n",
      "  Prediction:    ÙˆÙ‚Øª Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¹Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©\n",
      "  WER: 1.0000, CER: 0.8929\n",
      "\n",
      "Sample 2:\n",
      "  Ground Truth: dan kabebasan dari raso takuik dan dari kakurangan\n",
      "  Prediction:   dan kalaki nan parangakiranjan à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki à¥Œpaki\n",
      "  WER: 11.2500, CER: 10.4400\n",
      "\n",
      "Sample 3:\n",
      "  Ground Truth: supayo manjadi kanyataan\n",
      "  Prediction:   akimajal niyo\n",
      "  WER: 1.0000, CER: 0.7083\n",
      "\n",
      "Sample 4:\n",
      "  Ground Truth: indak ado pambedaan   umpamonyo pambedaan ras\n",
      "  Prediction:   à²•à³†à²°à³†à²¯à²¿à²•à³†à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¿à²¯à²¯à²¯à²¿à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯à²¯\n",
      "  WER: 1.0000, CER: 4.9333\n",
      "\n",
      "Sample 5:\n",
      "  Ground Truth: indak diadokan pambedaan badasar kadudukan politik\n",
      "  Prediction:    à²‡à²°à²¿à²¯à²¨à²¿à²¯à²¨à²¿à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦à³à²¦\n",
      "  WER: 1.0000, CER: 4.4400\n",
      "\n",
      "Sample 6:\n",
      "  Ground Truth: baiak babantuak negara mardeka\n",
      "  Prediction:    à²‡à²°à²¿à²¯à²¨à²¿à²¯à²¨à²¿à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦\n",
      "  WER: 1.0000, CER: 7.4000\n",
      "\n",
      "Sample 7:\n",
      "  Ground Truth: wilayah wilayah parwalian  jajahan atau bantuak katarbatasan kadaulatan nan lain\n",
      "  Prediction:    jajahan  atau  bantuak  katarbatasan  kadaulatan nan lain\n",
      "  WER: 0.3000, CER: 0.3875\n",
      "\n",
      "Sample 8:\n",
      "  Ground Truth: pasal tujuh sadonyo urang samo di hadapan hukum\n",
      "  Prediction:   Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆØ§Ù„Ø³Ø¹ÙˆØ¯Ø© ÙˆÙ…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©\n",
      "  WER: 1.0000, CER: 0.9362\n",
      "\n",
      "Sample 9:\n",
      "  Ground Truth: pasal sembilan indak surang pun buliah ditangkok  ditahan atau diasiangkan sacaro sawenang wenang\n",
      "  Prediction:   lansaki Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa Úˆa \n",
      "  WER: 8.5385, CER: 3.1340\n",
      "\n",
      "Sample 10:\n",
      "  Ground Truth: pasal sepuluh tiok urang punyo hak nan samo mandapek paradilan nan adil dan tabuka di pangadilan nan bebas dan tak mamihak\n",
      "  Prediction:    à²•à³†à²°à³à²¯à²¿à²•à³†à²¯à²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿\n",
      "  WER: 1.0000, CER: 1.3197\n",
      "\n",
      "Sample 11:\n",
      "  Ground Truth: nan bukan tamasuak palanggaran hukum nasional atau internasional katiko paristiwa itu tajadi\n",
      "  Prediction:    nans  à²‡à² à²¿à²¯à²¨à²¿à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†à²¦à³†ï¿½\n",
      "  WER: 1.0000, CER: 2.4239\n",
      "\n",
      "Sample 12:\n",
      "  Ground Truth: pasal empat belas tiok urang punyo hak mancari dan manikmati suaka di negara lain untuak malinduangi diri dari panganiayoan  di negaranyo\n",
      "  Prediction:   lansak linduangi diri Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡ Û¡\n",
      "  WER: 7.0476, CER: 2.0876\n",
      "\n",
      "Sample 13:\n",
      "  Ground Truth: bahati nurani dan ba agamo  tamasuak hak batuka agamo atau kaparcayaan\n",
      "  Prediction:    à²‡à²°à²¿à²•à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯à²¿à²¯à²¯à²¿à²¯\n",
      "  WER: 1.0000, CER: 3.1714\n",
      "\n",
      "Sample 14:\n",
      "  Ground Truth: tiok urang punyo hak mandapek kasampatan nan samo untuak diangkek dalam jabatan pamarintahan negaranyo\n",
      "  Prediction:    ÙˆÙ…Ù† ÙŠØ­Ø¨Ø¨ÙˆÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆØ§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©\n",
      "  WER: 1.0000, CER: 0.9314\n",
      "\n",
      "Sample 15:\n",
      "  Ground Truth: kandak rakyat musti manjadi landasan kakuasaan pamarintah\n",
      "  Prediction:    à¶…à¶´à¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Šà¶±à·Š\n",
      "  WER: 1.0000, CER: 3.8947\n",
      "\n",
      "Sample 16:\n",
      "  Ground Truth: tiok urang punyo hak mandapek jaminan sosial dan punyo hak pulo untuak talaksananyo hak hak ekonomi\n",
      "  Prediction:    à²•à³†à²°à³à²¯à²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²¿à²µà²µà²µà²¿à²µà²µà²µà²¿à²µà²µà²µà²¿à²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µà²µï¿½\n",
      "  WER: 1.0000, CER: 2.2525\n",
      "\n",
      "Sample 17:\n",
      "  Ground Truth: baitu pulo punyo hak mandapek linduangan katiko manganggur\n",
      "  Prediction:   lansak linduangahonyo å¿banyo  banso nan  sacaro jiniyak  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso  banso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro raso nan  sacaro nan  sacaro raso nan  sacaro nan \n",
      "  WER: 24.6250, CER: 19.8448\n",
      "\n",
      "Sample 18:\n",
      "  Ground Truth: tamasuak pambatasan nan layak taradok jam karajo\n",
      "  Prediction:    tÂ·tÊ°a Ê°i Ê°ak Ê°i Ê°ak Ê°i Ê°ak Ê°i Ê°ak Ê°i Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak Ê°ak ï¿½\n",
      "  WER: 15.8571, CER: 8.5833\n",
      "\n",
      "Sample 19:\n",
      "  Ground Truth: pandidikan dasar musti diwajibkan\n",
      "  Prediction:    à²•à³†à²°à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²¿à²•à³à²¯à²•à³à²¯à²¿à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³à²¯à²•à³\n",
      "  WER: 1.0000, CER: 7.8485\n",
      "\n",
      "Sample 20:\n",
      "  Ground Truth: pasal dua puluh delapan tiok urang punyo hak mandapek katartiban sosial dan katartiban internasional untuak mawujudkan saluruah hak hak dan kabebasan sasuai jo deklarasi ko\n",
      "  Prediction:    ÙˆÙ…Ù† ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ù‚Ù‚ Ø§Ù„Ù…Ø¯ÙÙ‚Ø© ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¥Ù†ØªØ±Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø³ÙˆØ§ØªÙŠØ© ÙˆØ§Ù„Ø³ÙˆØ§ØªÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø© Ù…Ù†Ù‡Ø§\n",
      "  WER: 1.0000, CER: 0.9419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Calculate Evaluation Metrics ---\n",
    "try:\n",
    "    import jiwer\n",
    "    \n",
    "    # Filter out None predictions\n",
    "    valid_pairs = [(pred, gt) for pred, gt in zip(predictions, ground_truths) if pred is not None]\n",
    "    \n",
    "    if valid_pairs:\n",
    "        valid_predictions, valid_ground_truths = zip(*valid_pairs)\n",
    "        \n",
    "        # Convert tuples to lists for jiwer\n",
    "        valid_predictions = list(valid_predictions)\n",
    "        valid_ground_truths = list(valid_ground_truths)\n",
    "        \n",
    "        # Calculate Word Error Rate (WER)\n",
    "        wer_score = jiwer.wer(valid_ground_truths, valid_predictions)\n",
    "        \n",
    "        # Calculate Character Error Rate (CER)\n",
    "        cer_score = jiwer.cer(valid_ground_truths, valid_predictions)\n",
    "        \n",
    "        print(\"ðŸ“Š EVALUATION RESULTS:\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"ðŸ“ Number of samples: {len(valid_pairs)}\")\n",
    "        print(f\"ðŸŽ¯ Word Error Rate (WER): {wer_score:.4f} ({wer_score*100:.2f}%)\")\n",
    "        print(f\"ðŸ”¤ Character Error Rate (CER): {cer_score:.4f} ({cer_score*100:.2f}%)\")\n",
    "        print(f\"âœ… Word Accuracy: {(1-wer_score)*100:.2f}%\")\n",
    "        \n",
    "        # Show detailed comparison\n",
    "        print(f\"\\nðŸ“‹ DETAILED COMPARISON:\")\n",
    "        print(\"=\"*50)\n",
    "        for i, (pred, gt) in enumerate(valid_pairs):\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"  Ground Truth: {gt}\")\n",
    "            print(f\"  Prediction:   {pred}\")\n",
    "            \n",
    "            # Calculate individual metrics\n",
    "            individual_wer = jiwer.wer([gt], [pred])\n",
    "            individual_cer = jiwer.cer([gt], [pred])\n",
    "            print(f\"  WER: {individual_wer:.4f}, CER: {individual_cer:.4f}\")\n",
    "            print()\n",
    "            \n",
    "    else:\n",
    "        print(\"âŒ No valid predictions to evaluate\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ jiwer not available. Install with: pip install jiwer\")\n",
    "    print(\"Showing basic comparison instead...\")\n",
    "    \n",
    "    for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):\n",
    "        if pred is not None:\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"  Ground Truth: {gt}\")\n",
    "            print(f\"  Prediction:   {pred}\")\n",
    "            print(f\"  Match: {'âœ…' if pred.lower().strip() == gt.lower().strip() else 'âŒ'}\")\n",
    "            print()\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error calculating metrics: {e}\")\n",
    "    print(\"Showing basic comparison instead...\")\n",
    "    \n",
    "    for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):\n",
    "        if pred is not None:\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"  Ground Truth: {gt}\")\n",
    "            print(f\"  Prediction:   {pred}\")\n",
    "            print(f\"  Match: {'âœ…' if pred.lower().strip() == gt.lower().strip() else 'âŒ'}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0df38a30-4334-4941-9a92-96ff6a646669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤ Re-testing model with improved generation parameters...\n",
      "Processing 1/5: human_rights_un_min_sd_0009.wav\n",
      "  Ground Truth: katahui bana hak hak awak ko\n",
      "  Prediction:   kaluarga atau manusia atau malakukan di tiok urang sa kadalamuran daripadaan iduik nan basawakai?\n",
      "  ==================================================\n",
      "Processing 2/5: human_rights_un_min_sd_0016.wav\n",
      "  Ground Truth: dan kabebasan dari raso takuik dan dari kakurangan\n",
      "  Prediction:   kaæ°‘ana ireksi tiok urang punyo hak mardeka untuak nafkah jinih parkawinan nan damai ka khusambagian\n",
      "  ==================================================\n",
      "Processing 3/5: human_rights_un_min_sd_0028.wav\n",
      "  Ground Truth: supayo manjadi kanyataan\n",
      "  Prediction:   man sadism anna baiak kaya negaraan jo randunnan musti pasatudok manyukaienho\n",
      "  ==================================================\n",
      "Processing 4/5: human_rights_un_min_sd_0039.wav\n",
      "  Ground Truth: indak ado pambedaan   umpamonyo pambedaan ras\n",
      "  Prediction:   sakai jo pandiliranyo sabagai?\n",
      "  ==================================================\n",
      "Processing 5/5: human_rights_un_min_sd_0044.wav\n",
      "  Ground Truth: indak diadokan pambedaan badasar kadudukan politik\n",
      "  Prediction:   pasal dua belas tiok urang punyo hak mandapek kawarga negaraan\n",
      "  ==================================================\n",
      "\n",
      "âœ… Completed improved testing on 5 samples\n"
     ]
    }
   ],
   "source": [
    "# --- Improved Model Testing with Better Generation Parameters ---\n",
    "def transcribe_audio_improved(audio_path, model, processor):\n",
    "    \"\"\"Transcribe with improved generation parameters to avoid repetition\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != SAMPLING_RATE:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, SAMPLING_RATE)\n",
    "        waveform = waveform.mean(dim=0)  # Convert to mono\n",
    "        \n",
    "        # Process audio\n",
    "        inputs = processor(\n",
    "            audio=waveform.numpy(),\n",
    "            sampling_rate=SAMPLING_RATE,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move to GPU\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate transcription with improved parameters\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                inputs[\"input_features\"],\n",
    "                max_length=200,  # Reduced from 448\n",
    "                min_length=1,\n",
    "                num_beams=3,     # Increased from 1\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,  # Prevent repetition\n",
    "                repetition_penalty=1.2,   # Penalize repetition\n",
    "                length_penalty=1.0,\n",
    "                bad_words_ids=[[50257]],  # Avoid problematic tokens\n",
    "                forced_decoder_ids=None,\n",
    "                temperature=1.0\n",
    "            )\n",
    "        \n",
    "        # Decode transcription\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        return transcription.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with improved parameters on first 5 samples only\n",
    "test_sample_improved = test_df_clean.head(5).copy()\n",
    "print(\"ðŸŽ¤ Re-testing model with improved generation parameters...\")\n",
    "\n",
    "improved_predictions = []\n",
    "improved_ground_truths = []\n",
    "\n",
    "for idx, row in test_sample_improved.iterrows():\n",
    "    print(f\"Processing {idx+1}/5: {os.path.basename(row['wav_full_path'])}\")\n",
    "    \n",
    "    # Get prediction with improved parameters\n",
    "    prediction = transcribe_audio_improved(row[\"wav_full_path\"], model, processor)\n",
    "    ground_truth = row[\"sentence\"]\n",
    "    \n",
    "    improved_predictions.append(prediction)\n",
    "    improved_ground_truths.append(ground_truth)\n",
    "    \n",
    "    print(f\"  Ground Truth: {ground_truth}\")\n",
    "    print(f\"  Prediction:   {prediction}\")\n",
    "    print(\"  \" + \"=\"*50)\n",
    "\n",
    "print(f\"\\nâœ… Completed improved testing on {len(test_sample_improved)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c93cb333-a7bf-49a5-a3ce-e2ab9c0d7c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š IMPROVED EVALUATION RESULTS:\n",
      "============================================================\n",
      "ðŸ“ Number of valid samples: 5\n",
      "ðŸŽ¯ Word Error Rate (WER): 1.9310 (193.10%)\n",
      "ðŸ”¤ Character Error Rate (CER): 1.4112 (141.12%)\n",
      "âœ… Word Accuracy: -93.10%\n",
      "ðŸ“ˆ Character Accuracy: -41.12%\n",
      "\n",
      "ðŸ“‹ DETAILED COMPARISON (IMPROVED):\n",
      "============================================================\n",
      "Sample 1:\n",
      "  Ground Truth: katahui bana hak hak awak ko\n",
      "  Prediction:   kaluarga atau manusia atau malakukan di tiok urang sa kadalamuran daripadaan iduik nan basawakai?\n",
      "  ðŸ“Š WER: 2.3333, CER: 2.7143\n",
      "  âŒ Needs improvement\n",
      "\n",
      "Sample 2:\n",
      "  Ground Truth: dan kabebasan dari raso takuik dan dari kakurangan\n",
      "  Prediction:   kaæ°‘ana ireksi tiok urang punyo hak mardeka untuak nafkah jinih parkawinan nan damai ka khusambagian\n",
      "  ðŸ“Š WER: 1.8750, CER: 1.3600\n",
      "  âŒ Needs improvement\n",
      "\n",
      "Sample 3:\n",
      "  Ground Truth: supayo manjadi kanyataan\n",
      "  Prediction:   man sadism anna baiak kaya negaraan jo randunnan musti pasatudok manyukaienho\n",
      "  ðŸ“Š WER: 3.6667, CER: 2.5000\n",
      "  âŒ Needs improvement\n",
      "\n",
      "Sample 4:\n",
      "  Ground Truth: indak ado pambedaan   umpamonyo pambedaan ras\n",
      "  Prediction:   sakai jo pandiliranyo sabagai?\n",
      "  ðŸ“Š WER: 1.0000, CER: 0.6667\n",
      "  âŒ Needs improvement\n",
      "\n",
      "Sample 5:\n",
      "  Ground Truth: indak diadokan pambedaan badasar kadudukan politik\n",
      "  Prediction:   pasal dua belas tiok urang punyo hak mandapek kawarga negaraan\n",
      "  ðŸ“Š WER: 1.6667, CER: 0.8800\n",
      "  âŒ Needs improvement\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Calculate Improved Evaluation Metrics ---\n",
    "try:\n",
    "    import jiwer\n",
    "    \n",
    "    # Filter out None predictions from improved results\n",
    "    valid_pairs_improved = [(pred, gt) for pred, gt in zip(improved_predictions, improved_ground_truths) if pred is not None and pred.strip() != \"\"]\n",
    "    \n",
    "    if valid_pairs_improved:\n",
    "        valid_predictions_improved, valid_ground_truths_improved = zip(*valid_pairs_improved)\n",
    "        \n",
    "        # Convert tuples to lists for jiwer\n",
    "        valid_predictions_improved = list(valid_predictions_improved)\n",
    "        valid_ground_truths_improved = list(valid_ground_truths_improved)\n",
    "        \n",
    "        # Calculate Word Error Rate (WER)\n",
    "        wer_score_improved = jiwer.wer(valid_ground_truths_improved, valid_predictions_improved)\n",
    "        \n",
    "        # Calculate Character Error Rate (CER)\n",
    "        cer_score_improved = jiwer.cer(valid_ground_truths_improved, valid_predictions_improved)\n",
    "        \n",
    "        print(\"ðŸ“Š IMPROVED EVALUATION RESULTS:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ðŸ“ Number of valid samples: {len(valid_pairs_improved)}\")\n",
    "        print(f\"ðŸŽ¯ Word Error Rate (WER): {wer_score_improved:.4f} ({wer_score_improved*100:.2f}%)\")\n",
    "        print(f\"ðŸ”¤ Character Error Rate (CER): {cer_score_improved:.4f} ({cer_score_improved*100:.2f}%)\")\n",
    "        print(f\"âœ… Word Accuracy: {(1-wer_score_improved)*100:.2f}%\")\n",
    "        print(f\"ðŸ“ˆ Character Accuracy: {(1-cer_score_improved)*100:.2f}%\")\n",
    "        \n",
    "        # Show detailed comparison\n",
    "        print(f\"\\nðŸ“‹ DETAILED COMPARISON (IMPROVED):\")\n",
    "        print(\"=\"*60)\n",
    "        for i, (pred, gt) in enumerate(valid_pairs_improved):\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"  Ground Truth: {gt}\")\n",
    "            print(f\"  Prediction:   {pred}\")\n",
    "            \n",
    "            # Calculate individual metrics\n",
    "            individual_wer = jiwer.wer([gt], [pred])\n",
    "            individual_cer = jiwer.cer([gt], [pred])\n",
    "            print(f\"  ðŸ“Š WER: {individual_wer:.4f}, CER: {individual_cer:.4f}\")\n",
    "            \n",
    "            # Show if it's a good prediction\n",
    "            if individual_wer < 0.3:\n",
    "                print(f\"  âœ… Good transcription!\")\n",
    "            elif individual_wer < 0.6:\n",
    "                print(f\"  âš ï¸ Moderate accuracy\")\n",
    "            else:\n",
    "                print(f\"  âŒ Needs improvement\")\n",
    "            print()\n",
    "            \n",
    "    else:\n",
    "        print(\"âŒ No valid predictions to evaluate in improved results\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error calculating improved metrics: {e}\")\n",
    "    print(\"Showing basic comparison instead...\")\n",
    "    \n",
    "    for i, (pred, gt) in enumerate(zip(improved_predictions, improved_ground_truths)):\n",
    "        if pred is not None and pred.strip() != \"\":\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"  Ground Truth: {gt}\")\n",
    "            print(f\"  Prediction:   {pred}\")\n",
    "            print(f\"  Match: {'âœ…' if pred.lower().strip() == gt.lower().strip() else 'âŒ'}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b055734-24fb-4883-8633-3ada0ca318e7",
   "metadata": {},
   "source": [
    "# Model Analysis and Improvement Suggestions\n",
    "This section analyzes the model performance and provides insights into potential improvements. The results show that while the model has learned some Minangkabau patterns, it needs more training or different hyperparameters to achieve better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c769168e-0b4b-4ee2-a18b-be18165ebaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” PERFORMANCE ANALYSIS:\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Current Model Performance:\n",
      "   â€¢ Word Error Rate: 193.1%\n",
      "   â€¢ Character Error Rate: 141.1%\n",
      "   â€¢ Word Accuracy: -93.1%\n",
      "   â€¢ Character Accuracy: -41.1%\n",
      "\n",
      "ðŸŽ¯ Accuracy Interpretation:\n",
      "   âŒ High WER (>80%): Model needs significant improvement\n",
      "\n",
      "ðŸ”¤ Character-level Analysis:\n",
      "   âŒ Poor character accuracy - limited phonetic learning\n",
      "\n",
      "ðŸ“ˆ Training Insights:\n",
      "   â€¢ Short training (3 epochs, 12 steps) - likely underfitted\n",
      "   â€¢ Model shows some Minangkabau patterns but needs more exposure\n",
      "   â€¢ Character-level performance better than word-level suggests partial learning\n",
      "\n",
      "ðŸ› ï¸ Improvement Strategies:\n",
      "   1. Increase training epochs (5-10 epochs)\n",
      "   2. Lower learning rate for fine-grained learning\n",
      "   3. Increase dataset size if possible\n",
      "   4. Use data augmentation (speed/pitch variations)\n",
      "   5. Fine-tune generation parameters further\n",
      "   6. Consider using Whisper-base instead of small for better capacity\n"
     ]
    }
   ],
   "source": [
    "# --- Performance Analysis ---\n",
    "print(\"ðŸ” PERFORMANCE ANALYSIS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate basic statistics from the improved results\n",
    "if 'valid_pairs_improved' in locals() and valid_pairs_improved:\n",
    "    print(f\"\\nðŸ“Š Current Model Performance:\")\n",
    "    print(f\"   â€¢ Word Error Rate: {wer_score_improved*100:.1f}%\")\n",
    "    print(f\"   â€¢ Character Error Rate: {cer_score_improved*100:.1f}%\") \n",
    "    print(f\"   â€¢ Word Accuracy: {(1-wer_score_improved)*100:.1f}%\")\n",
    "    print(f\"   â€¢ Character Accuracy: {(1-cer_score_improved)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Accuracy Interpretation:\")\n",
    "    if wer_score_improved > 0.8:\n",
    "        print(\"   âŒ High WER (>80%): Model needs significant improvement\")\n",
    "    elif wer_score_improved > 0.5:\n",
    "        print(\"   âš ï¸ Moderate WER (50-80%): Model shows some learning but needs refinement\")\n",
    "    elif wer_score_improved > 0.3:\n",
    "        print(\"   ðŸ”¶ Fair WER (30-50%): Model is learning patterns well\")\n",
    "    else:\n",
    "        print(\"   âœ… Good WER (<30%): Model performing well\")\n",
    "        \n",
    "    print(f\"\\nðŸ”¤ Character-level Analysis:\")\n",
    "    if cer_score_improved < 0.3:\n",
    "        print(\"   âœ… Good character accuracy - model understands phonetics\")\n",
    "    elif cer_score_improved < 0.6:\n",
    "        print(\"   ðŸ”¶ Moderate character accuracy - some phonetic understanding\")\n",
    "    else:\n",
    "        print(\"   âŒ Poor character accuracy - limited phonetic learning\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Training Insights:\")\n",
    "print(\"   â€¢ Short training (3 epochs, 12 steps) - likely underfitted\")\n",
    "print(\"   â€¢ Model shows some Minangkabau patterns but needs more exposure\")\n",
    "print(\"   â€¢ Character-level performance better than word-level suggests partial learning\")\n",
    "\n",
    "print(f\"\\nðŸ› ï¸ Improvement Strategies:\")\n",
    "print(\"   1. Increase training epochs (5-10 epochs)\")\n",
    "print(\"   2. Lower learning rate for fine-grained learning\")\n",
    "print(\"   3. Increase dataset size if possible\")\n",
    "print(\"   4. Use data augmentation (speed/pitch variations)\")\n",
    "print(\"   5. Fine-tune generation parameters further\")\n",
    "print(\"   6. Consider using Whisper-base instead of small for better capacity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba259405-820b-4c19-9516-7a4309455b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ SAVING FINE-TUNED MODEL:\n",
      "==================================================\n",
      "Saving model to: ./whisper-minang-final\n",
      "âœ… Model and processor saved successfully!\n",
      "ðŸ“ Location: ./whisper-minang-final\n",
      "ðŸ“Š Training results summary saved!\n",
      "\n",
      "ðŸ”„ To load this model later, use:\n",
      "```python\n",
      "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
      "processor = WhisperProcessor.from_pretrained('./whisper-minang-final')\n",
      "model = WhisperForConditionalGeneration.from_pretrained('./whisper-minang-final')\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# --- Save Model for Future Use ---\n",
    "print(\"ðŸ’¾ SAVING FINE-TUNED MODEL:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Save the fine-tuned model and processor\n",
    "    model_save_path = \"./whisper-minang-final\"\n",
    "    \n",
    "    print(f\"Saving model to: {model_save_path}\")\n",
    "    model.save_pretrained(model_save_path)\n",
    "    processor.save_pretrained(model_save_path)\n",
    "    \n",
    "    print(\"âœ… Model and processor saved successfully!\")\n",
    "    print(f\"ðŸ“ Location: {model_save_path}\")\n",
    "    \n",
    "    # Save a summary of the training results\n",
    "    results_summary = {\n",
    "        \"model_name\": \"whisper-small-minangkabau\",\n",
    "        \"training_epochs\": 3,\n",
    "        \"training_steps\": 12,\n",
    "        \"final_loss\": 5.78,\n",
    "        \"test_samples\": len(valid_pairs_improved) if 'valid_pairs_improved' in locals() else 0,\n",
    "        \"word_error_rate\": f\"{wer_score_improved*100:.2f}%\" if 'wer_score_improved' in locals() else \"N/A\",\n",
    "        \"character_error_rate\": f\"{cer_score_improved*100:.2f}%\" if 'cer_score_improved' in locals() else \"N/A\",\n",
    "        \"word_accuracy\": f\"{(1-wer_score_improved)*100:.2f}%\" if 'wer_score_improved' in locals() else \"N/A\",\n",
    "        \"character_accuracy\": f\"{(1-cer_score_improved)*100:.2f}%\" if 'cer_score_improved' in locals() else \"N/A\"\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(f\"{model_save_path}/training_results.json\", \"w\") as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(\"ðŸ“Š Training results summary saved!\")\n",
    "    \n",
    "    # Instructions for loading the model later\n",
    "    print(f\"\\nðŸ”„ To load this model later, use:\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\")\n",
    "    print(f\"processor = WhisperProcessor.from_pretrained('{model_save_path}')\")\n",
    "    print(f\"model = WhisperForConditionalGeneration.from_pretrained('{model_save_path}')\")\n",
    "    print(f\"```\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving model: {e}\")\n",
    "    print(\"The model is still available in memory for this session.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
